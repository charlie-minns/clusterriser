{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import astropy.table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
    "import btk\n",
    "import btk.config, btk.plot_utils\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for generating galaxies: https://github.com/LSSTDESC/BlendingToolKit/blob/master/notebooks/custom_sampling_function.ipynb\n",
    "# input catalog name\n",
    "catalog_name = os.path.join(os.path.dirname(os.getcwd()), 'data', 'sample_input_catalog.fits')\n",
    "\n",
    "# load parameters\n",
    "# max_number = maximum number of galaxies in image (10), batch_size = number of images (100)\n",
    "param = btk.config.Simulation_params(catalog_name, max_number=10, batch_size=5)\n",
    "np.random.seed(param.seed)\n",
    "\n",
    "# load input catalog\n",
    "catalog = btk.get_input_catalog.load_catalog(param)\n",
    "\n",
    "# generate catalogs of blended objects \n",
    "blend_generator = btk.create_blend_generator.generate(param, catalog)\n",
    "\n",
    "# generates observing conditions for the selected survey_name and all input bands\n",
    "observing_generator = btk.create_observing_generator.generate(param)\n",
    "\n",
    "# generate images of blends in all the observing bands\n",
    "draw_blend_generator = btk.draw_blends.generate(param, blend_generator, observing_generator)\n",
    "\n",
    "# generates new batch_size number of blends\n",
    "blend_results = next(draw_blend_generator)\n",
    "output = blend_results\n",
    "blend_images = output['blend_images']\n",
    "isolated_images = output['isolated_images']\n",
    "blend_list = output['blend_list']\n",
    "obs_cond = output['obs_condition']\n",
    "\n",
    "# plot blended images\n",
    "plot = False\n",
    "if (plot): btk.plot_utils.plot_blends(blend_images[0:10], blend_list[0:10], limits=(30,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the background and noise of the images\n",
    "sky_level = []\n",
    "for oc in obs_cond[0]: # same values of each obs_cond?\n",
    "    sky_level.append(oc.mean_sky_level)\n",
    "background = np.array(sky_level).sum()\n",
    "std_background = np.sqrt(background)\n",
    "\n",
    "# make histogram\n",
    "n = 120\n",
    "total = []\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        total.append(blend_images[0][i, j].sum())\n",
    "n, bins = np.histogram(total)\n",
    "mids = 0.5*(bins[1:] + bins[:-1])\n",
    "mean = np.average(mids, weights=n)\n",
    "var = np.average((mids - mean)**2, weights=n)\n",
    "std = np.sqrt(var)\n",
    "\n",
    "print(\"Histogram: \" + str(std))\n",
    "print(\"Observing conditions: \" + str(std_background))\n",
    "plt.hist(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average magnitudes of galaxies in catalog\n",
    "magnitudes = []\n",
    "bulge_magnitudes = []\n",
    "for img in blend_list:\n",
    "    mags = []\n",
    "    for gal in img:\n",
    "        mag = gal['i_ab'] \n",
    "        mags.append(mag)\n",
    "        \n",
    "        # colour = gal['g_ab'] - gal['i_ab']\n",
    "\n",
    "    magnitudes.append(np.mean(mags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the detection efficiency of the HDBSCAN for a set of images with known numbers of clusters\n",
    "# function of minimum cluster size parameter\n",
    "\n",
    "# using parameter values from 5-50, in increments of 5\n",
    "min_cluster_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "# the detection efficiency using 'eom' on raw data\n",
    "eom_detection_efficiency = []\n",
    "# the detection efficiency using 'eom' on normalised data\n",
    "eom_norm_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on raw data\n",
    "leaf_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on normalised data\n",
    "leaf_norm_detection_efficiency = []\n",
    "\n",
    "# find the detection efficiency for each value used for the minimum cluster size parameter\n",
    "for mcs in min_cluster_sizes:\n",
    "    eom_efficiency = []\n",
    "    eom_norm_efficiency = []\n",
    "    leaf_efficiency = []\n",
    "    leaf_norm_efficiency = []\n",
    "    \n",
    "    # get results using both 'eom' and 'leaf' cluster selection methods\n",
    "    eom_clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs)\n",
    "    leaf_clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='leaf') \n",
    "\n",
    "    # get the average detection efficiency from each image\n",
    "    for i in range(len(blend_list)):\n",
    "        # get the actual number of clusters in this image\n",
    "        true_centers = np.stack([blend_list[i]['dx'], blend_list[i]['dy']]).T\n",
    "        actual_k = len(true_centers)\n",
    "                \n",
    "        # put each band along with x and y dimensions into one array\n",
    "        img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "        n = 120\n",
    "        x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "        arrays = [x.flatten(), y.flatten()]\n",
    "        for j in range(6):\n",
    "            arrays.append(img[:, j])\n",
    "        data = np.stack(arrays, axis=1)\n",
    "\n",
    "        # normalise data\n",
    "        norm_data = data.astype('float') #- background\n",
    "        norm_data /= (np.maximum(std_background, data.sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "    \n",
    "        # find number of clusters detected for raw data using 'eom'\n",
    "        eom_clusterer.fit(data)\n",
    "        detected_k = max(eom_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for normalised data using 'eom'\n",
    "        eom_clusterer.fit(norm_data)\n",
    "        detected_k = max(eom_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_norm_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for raw data using 'leaf'\n",
    "        leaf_clusterer.fit(data)\n",
    "        detected_k = max(leaf_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for normalised data using 'leaf'\n",
    "        leaf_clusterer.fit(data)\n",
    "        detected_k = max(leaf_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_norm_efficiency.append(detected_k/actual_k)\n",
    "    \n",
    "    # take the efficiency to be the mean of the number of clusters detected divided by the actual number of clusters for each image\n",
    "    eom_detection_efficiency.append(np.mean(eom_efficiency))\n",
    "    eom_norm_detection_efficiency.append(np.mean(eom_norm_efficiency))\n",
    "    leaf_detection_efficiency.append(np.mean(leaf_efficiency))\n",
    "    leaf_norm_detection_efficiency.append(np.mean(leaf_norm_efficiency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of the minimum cluster size parameter\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(min_cluster_sizes, eom_detection_efficiency)\n",
    "eom_norm_sc = eom_norm_ax.scatter(min_cluster_sizes, eom_norm_detection_efficiency)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(min_cluster_sizes, leaf_detection_efficiency)\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(min_cluster_sizes, leaf_norm_detection_efficiency)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the detection efficiency of the HDBSCAN for a set of images with known numbers of clusters\n",
    "# function of number of clusters in imagee\n",
    "\n",
    "# using parameter values from 5-50, in increments of 5\n",
    "min_cluster_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "# the number of clusters in each image\n",
    "clusters = [] \n",
    "all_clusters = []\n",
    "# the detection efficiency using 'eom' on raw data\n",
    "eom_detection_efficiency = []\n",
    "total_eom_detection_efficiency = []\n",
    "# the detection efficiency using 'eom' on normalised data\n",
    "eom_norm_detection_efficiency = []\n",
    "total_eom_norm_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on raw data\n",
    "leaf_detection_efficiency = []\n",
    "total_leaf_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on normalised data\n",
    "leaf_norm_detection_efficiency = []\n",
    "total_leaf_norm_detection_efficiency = []\n",
    "\n",
    "# find the average number of clusters detected using HDBSCAN for each image\n",
    "for i in range(len(blend_list)):\n",
    "    eom_efficiency = []\n",
    "    eom_norm_efficiency = []\n",
    "    leaf_efficiency = []\n",
    "    leaf_norm_efficiency = []\n",
    "    \n",
    "    # put each band along with x and y dimensions into one array\n",
    "    img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "    x, y = np.meshgrid(0.1*np.arange(120), 0.1*np.arange(120))\n",
    "    arrays = [x.flatten(), y.flatten()]\n",
    "    for j in range(6):\n",
    "        arrays.append(img[:, j])\n",
    "    data = np.stack(arrays, axis=1)\n",
    "\n",
    "    # normalise data\n",
    "    norm_data = data.astype('float') #- background # works much better without subtracting the background- why? \n",
    "    norm_data /= (np.maximum(std_background, data.sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "\n",
    "    # get the actual number of clusters from the image\n",
    "    true_centers = np.stack([blend_list[i]['dx'], blend_list[i]['dy']]).T\n",
    "    actual_k = len(true_centers)\n",
    "    clusters.append(actual_k)\n",
    "\n",
    "    # use HDBSCAN to estimate the number of clusters using a series of values for the minimum cluster size parameter\n",
    "    for mcs in min_cluster_sizes:\n",
    "        all_clusters.append(actual_k)\n",
    "        \n",
    "        # clustering using HDBSCAN with the 'eom' selection method\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='eom')\n",
    "        \n",
    "        # find the number of clusters using raw data\n",
    "        clusterer.fit(data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_efficiency.append(detected_k)\n",
    "        total_eom_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find the number of clusters using normalised data\n",
    "        clusterer.fit(norm_data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_norm_efficiency.append(detected_k)\n",
    "        total_eom_norm_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # clustering using HDBSCAN with the 'leaf' selection method\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='leaf')\n",
    "        \n",
    "        # find the number of clusters using raw data\n",
    "        clusterer.fit(data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1 \n",
    "        leaf_efficiency.append(detected_k)\n",
    "        total_leaf_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find the number of clusters using normalised data\n",
    "        clusterer.fit(norm_data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_norm_efficiency.append(detected_k)\n",
    "        total_leaf_norm_detection_efficiency.append(detected_k/actual_k)\n",
    "    \n",
    "    # take the efficiency to be the mean of the number of clusters detected divided by the actual number of clusters for every image\n",
    "    eom_detection_efficiency.append(np.mean(eom_efficiency)/actual_k)\n",
    "    eom_norm_detection_efficiency.append(np.mean(eom_norm_efficiency)/actual_k)\n",
    "    leaf_detection_efficiency.append(np.mean(leaf_efficiency)/actual_k)\n",
    "    leaf_norm_detection_efficiency.append(np.mean(leaf_norm_efficiency)/actual_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of actual number of clusters\n",
    "# using a colour map to show the average magnitude of the galaxies in an image\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(clusters, eom_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "eom_norm_sc = eom_norm_ax.scatter(clusters, eom_norm_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(eom_sc, ax=eom_ax)\n",
    "eom_cb = plt.colorbar(eom_norm_sc, ax=eom_norm_ax)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(clusters, leaf_detection_efficiency, c=magnitudes, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(clusters, leaf_norm_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(leaf_sc, ax=leaf_ax)\n",
    "leaf_cb = plt.colorbar(leaf_norm_sc, ax=leaf_norm_ax)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_cb.set_label(\"Average Magnitude of Clusters\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_cb.set_label(\"Average Magnitude of Clusters\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of actual number of clusters for every parameter value\n",
    "# using a colour map to show the minimum cluster size parameter\n",
    "cm = plt.cm.get_cmap('Spectral')\n",
    "min_cluster_vals = [mc for mc in min_cluster_sizes for i in range(len(blend_list))]\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(all_clusters, total_eom_detection_efficiency, c=min_cluster_vals, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "eom_norm_sc = eom_norm_ax.scatter(all_clusters, total_eom_norm_detection_efficiency, c=min_cluster_vals, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(eom_sc, ax=eom_ax)\n",
    "eom_cb = plt.colorbar(eom_norm_sc, ax=eom_norm_ax)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(all_clusters, total_leaf_detection_efficiency, c=min_cluster_vals, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(all_clusters, total_leaf_norm_detection_efficiency, c=min_cluster_vals, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(leaf_sc, ax=leaf_ax)\n",
    "leaf_cb = plt.colorbar(leaf_norm_sc, ax=leaf_norm_ax)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_cb.set_label(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_cb.set_label(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background, std_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = blend_images[0:1][0].reshape(-1, 6)\n",
    "n = 120\n",
    "x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "arrays = [x.flatten(), y.flatten()]\n",
    "for j in range(6):\n",
    "    arrays.append(img[:, j])\n",
    "data = np.stack(arrays, axis=1)\n",
    "plt.imshow(img.reshape(n, n, 6)[:, :, 0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the clustering results for 5 different images\n",
    "\n",
    "# code for generating galaxies: https://github.com/LSSTDESC/BlendingToolKit/blob/master/notebooks/custom_sampling_function.ipynb\n",
    "# input catalog name\n",
    "catalog_name = os.path.join(os.path.dirname(os.getcwd()), 'data', 'sample_input_catalog.fits')\n",
    "\n",
    "# load parameters\n",
    "param = btk.config.Simulation_params(catalog_name, max_number=10, batch_size=5)\n",
    "np.random.seed(param.seed)\n",
    "\n",
    "# load input catalog\n",
    "catalog = btk.get_input_catalog.load_catalog(param)\n",
    "\n",
    "# generate catalogs of blended objects \n",
    "blend_generator = btk.create_blend_generator.generate(param, catalog)\n",
    "\n",
    "# generates observing conditions for the selected survey_name and all input bands\n",
    "observing_generator = btk.create_observing_generator.generate(param)\n",
    "\n",
    "# generate images of blends in all the observing bands\n",
    "draw_blend_generator = btk.draw_blends.generate(param, blend_generator, observing_generator)\n",
    "\n",
    "# generates new batch_size number of blends\n",
    "blend_results = next(draw_blend_generator)\n",
    "output = blend_results\n",
    "blend_images = output['blend_images']\n",
    "isolated_images = output['isolated_images']\n",
    "blend_list = output['blend_list']\n",
    "obs_cond = output['obs_condition']\n",
    "\n",
    "# get clustering results using 'leaf' selection method and a minimum cluster size of 10 for normalised data\n",
    "for i in range(len(blend_list)):\n",
    "    # put each band along with x and y dimensions into one array\n",
    "    img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "    n = 120\n",
    "    x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "    arrays = [x.flatten(), y.flatten()]\n",
    "    for j in range(6):\n",
    "        arrays.append(img[:, j])\n",
    "    data = np.stack(arrays, axis=1)\n",
    "\n",
    "    # normalise data\n",
    "    norm_data = data.astype('float') #- background # works much better without subtracting the background- why? \n",
    "    mask = norm_data[:, 2:].sum(axis=1) < std_background\n",
    "    norm_data[:, 2:] /= (np.maximum(std_background, norm_data[:, 2:].sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "    norm_data[:, 2:][mask] = 0\n",
    "    #norm_data[:, :2] /= n\n",
    "\n",
    "    # use HDBSCAN to get a clustering result\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=10, cluster_selection_method='leaf')\n",
    "    clusterer.fit(norm_data)\n",
    "    labels = clusterer.labels_.reshape(n, n)\n",
    "    \n",
    "    # plot images\n",
    "    fig = plt.figure()\n",
    "    ax_data = fig.add_subplot(121)\n",
    "    ax_hdb = fig.add_subplot(122)\n",
    "    ax_data.imshow(blend_images[i:i+1][0][:, :, :].sum(axis=-1), origin='lower')\n",
    "    #ax_data.imshow(norm_data.sum(axis=-1).reshape(n,n), origin='lower')\n",
    "    ax_hdb.imshow(labels, origin='lower')\n",
    "    #ax_hdb.hist(norm_data.sum(axis=-1).flatten(), bins=100)\n",
    "    \n",
    "    #btk.plot_utils.plot_blends(blend_images[i:i+1], blend_list[i:i+1], limits=(30,90))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btk.plot_utils.plot_blends(blend_images, blend_list, limits=(30,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
