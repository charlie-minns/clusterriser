{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import astropy.table\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
    "import btk\n",
    "import btk.config, btk.plot_utils\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for generating galaxies: https://github.com/LSSTDESC/BlendingToolKit/blob/master/notebooks/custom_sampling_function.ipynb\n",
    "# input catalog name\n",
    "catalog_name = os.path.join('data', 'sample_input_catalog.fits')\n",
    "\n",
    "# load parameters\n",
    "# max_number = maximum number of galaxies in image (10), batch_size = number of images (100)\n",
    "param = btk.config.Simulation_params(catalog_name, max_number=10, batch_size=100)\n",
    "np.random.seed(param.seed)\n",
    "\n",
    "# load input catalog\n",
    "catalog = btk.get_input_catalog.load_catalog(param)\n",
    "\n",
    "# generate catalogs of blended objects \n",
    "blend_generator = btk.create_blend_generator.generate(param, catalog)\n",
    "\n",
    "# generates observing conditions for the selected survey_name and all input bands\n",
    "observing_generator = btk.create_observing_generator.generate(param)\n",
    "\n",
    "# generate images of blends in all the observing bands\n",
    "draw_blend_generator = btk.draw_blends.generate(param, blend_generator, observing_generator)\n",
    "\n",
    "# generates new batch_size number of blends\n",
    "blend_results = next(draw_blend_generator)\n",
    "output = blend_results\n",
    "blend_images = output['blend_images']\n",
    "isolated_images = output['isolated_images']\n",
    "blend_list = output['blend_list']\n",
    "obs_cond = output['obs_condition']\n",
    "\n",
    "# plot blended images\n",
    "plot = False\n",
    "if (plot): btk.plot_utils.plot_blends(blend_images[0:10], blend_list[0:10], limits=(30,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting them in multi-color\n",
    "btk.plot_utils.plot_blends(blend_images[:20], blend_list[:20], limits=(30,90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reported noise level in the image conforms well with their actual histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the background and noise of the images\n",
    "sky_level = []\n",
    "for oc in obs_cond[0]: # same values of each obs_cond?\n",
    "    sky_level.append(oc.mean_sky_level)\n",
    "background = np.array(sky_level)\n",
    "std_background = np.sqrt(background)\n",
    "std_sum = np.sqrt((std_background**2).sum())\n",
    "\n",
    "# show histogram comparison, single band only\n",
    "bins = np.linspace(-3*std_background[0], 3*std_background[0], 50)\n",
    "plt.hist(blend_images[:,:,:,0].flatten(), bins=bins, density=True);\n",
    "from scipy.stats import norm\n",
    "plt.plot(bins, norm.pdf(bins, scale=std_background[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same intensity normalization we've been using before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_channels(img, std_sum):\n",
    "    # normalise data: sum normalization for band amplitudes\n",
    "    # prevent division by zero (or close to) by cutting normalization off at noise level\n",
    "    return img / np.maximum(std_sum, img.sum(axis=-1))[:,:,None]\n",
    "\n",
    "def prepare_data(img, threshold, alpha=1):\n",
    "    # select pixels whose sum is above threshold,\n",
    "    # normalize their intensities, and xy values multiplied with alpha to extend feature vector\n",
    "    Ny, Nx, C = img.shape\n",
    "    mask = img.sum(axis=-1) > threshold\n",
    "    img_ = normalize_channels(img, threshold)\n",
    "    \n",
    "    x, y = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "    arrays = [alpha * x.flatten(), alpha * y.flatten()] + [img_[:,:,c].flatten() for c in range(C)]\n",
    "    data = np.stack(arrays, axis=1)\n",
    "    return data, mask\n",
    "\n",
    "\n",
    "i = 0\n",
    "Ny, Nx, C = blend_images[i].shape\n",
    "mcs = 10\n",
    "data, mask = prepare_data(blend_images[i], std_sum * 3, alpha=0.01)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=5, cluster_selection_method='leaf', allow_single_cluster=True)\n",
    "labels = clusterer.fit_predict(data)\n",
    "uniq_labels = np.unique(labels)\n",
    "print(uniq_labels)\n",
    "label_img = labels.reshape(mask.shape)\n",
    "\n",
    "# plot result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "axes[0].imshow(blend_images[i].sum(axis=-1))\n",
    "axes[0].scatter(blend_list[i]['dx'], blend_list[i]['dy'], color='r', marker='x')\n",
    "axes[1].imshow(label_img, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this approach is that the images are pretty noisy, which suggest high values of $\\alpha$ to dominate the pixel mutual distances for clustering. However, no setting of $\\alpha$ I've tried works well with bright and faint sources. Fundamentally, we need to modify the metric (which is by default Euclidean) to something more appropriate for this problem.\n",
    "\n",
    "I will now use a custom distance metric and precompute it, so that clustering directly sees the pairwise distance matrix.\n",
    "\n",
    "In particular, I compute distance as\n",
    "\n",
    "$$\n",
    "1 - r(v'_i,v'_j) + \\alpha^2 d(x_i, x_j)^2\n",
    "$$\n",
    "\n",
    "that is the combination of color term and a spatial distance term. The first is the Pearson correlation coefficient $r$ of the values (that is the band intensities) in pixels $i$ and $j$; the second the squared distance of their xy positions in the image, scaled by $\\alpha$ to allow for the relevant importance of spatial features to be altered.\n",
    "\n",
    "To standardize the results of the correlation coefficient, we need to account for the different variance in each channel, so\n",
    "\n",
    "$$\n",
    "v'_i = v_i / \\sigma_{bg}\n",
    "$$\n",
    "\n",
    "where the last terms is the vector of std of the background noise in each channel. This give a $N\\times N$ distance matrix, which will be passed to HDBSCAN.\n",
    "\n",
    "As success metric, we compute the Intersection-over-Union (a [standard measure](https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/) for detection algorithms) of the labeled pixels (ignoring the -1 outlier pixels) against the true footprints of detectable objects (pixels where the true image of the single source is above the detection threshold). The result is a matrix $Y$ with shape $N_{clustered} \\times N_{detectable}$, which is ideally close to the identity matrix.\n",
    "\n",
    "Because the clustering labels can come in any order and can have a different number as the detectable objects, we compute the cross-correlation matrix $C = Y^\\top Y$ of the labels with respect to the true indices. This is a common clustering metric (e.g. arXiv:1508.04306). The squared deviation from the identity $\\mathbb{1}_{N_{detectable}}$ is thus our optimization loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(img, threshold, alpha, normalization=None):\n",
    "    from scipy.stats import t as studentt\n",
    "    Ny, Nx, C = img.shape\n",
    "    \n",
    "    # work on pixels above threshold\n",
    "    mask = img.sum(axis=-1) > threshold\n",
    "    \n",
    "    if normalization is None:\n",
    "        _img = img\n",
    "        \n",
    "    else:\n",
    "        _img = img / normalization\n",
    "        \n",
    "    # compute Pearson r for color distance\n",
    "    r = np.corrcoef(_img[mask, :].reshape(-1, C))\n",
    "    \n",
    "    # compute pairwise Euclidean distance\n",
    "    x, y = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "    xy = np.stack((x[mask].flatten(), y[mask].flatten()), axis=1)\n",
    "    R2 = ((xy[:,None] - xy[None,:])**2).sum(axis=-1) / 2\n",
    "\n",
    "    # combine color and spatial distance, with alpha scaling\n",
    "    dist = (1 - r) + alpha**2 * R2\n",
    "    return dist, mask\n",
    "\n",
    "def iou_matrix(footprints, uniq_labels, label_img):\n",
    "    # compute intersection over union for every pair of true and clustered\n",
    "    has_object = footprints.any(axis=(1,2))\n",
    "    num_objects = has_object.sum()\n",
    "    if uniq_labels[0] == -1:\n",
    "        num_clustered = len(uniq_labels)-1\n",
    "    else:\n",
    "        num_clustered = len(uniq_labels)\n",
    "    \n",
    "    iou = np.zeros((num_clustered, num_objects))\n",
    "    for ll in uniq_labels:\n",
    "        if ll > -1:\n",
    "            fp_label = label_img == ll\n",
    "            for ii, fp_true in enumerate(footprints[has_object]):\n",
    "                union = (fp_true | fp_label).sum()\n",
    "                intersection = (fp_true & fp_label).sum()\n",
    "                norm = np.sqrt(fp_true.sum() * fp_label.sum())\n",
    "                iou_ = intersection / union\n",
    "                iou[ll][ii] = iou_\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "mcs = 5\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# alpha and threshold values from optimimzation below\n",
    "# can pick any positive number for alpha, and something of order std_sum for threshold\n",
    "if mcs == 10:\n",
    "    alpha, threshold = 1.65473326e-01, 2.80763024e+03 # 1.07263079e-01, 2.80763543e+03\n",
    "elif mcs == 5:\n",
    "    alpha, threshold = 1.53615598e-01, 2.80763018e+03\n",
    "    \n",
    "# all channels unit variance\n",
    "normalization =  std_background[None,None,:]\n",
    "    \n",
    "# compute detection mask and color/spatial distances\n",
    "X, mask = sim_matrix(blend_images[i], threshold, alpha, normalization=normalization)\n",
    "\n",
    "# cluster distance matrix\n",
    "labels = clusterer.fit_predict(X)\n",
    "uniq_labels = np.unique(labels)\n",
    "print(uniq_labels)\n",
    "label_img = np.ones(mask.shape) * -2\n",
    "label_img[mask] = labels\n",
    "\n",
    "# check overlap with true footprints\n",
    "footprints = isolated_images[i].sum(axis=-1) > std_sum\n",
    "has_object = footprints.any(axis=(1,2))\n",
    "num_objects = has_object.sum()\n",
    "Y = iou_matrix(footprints, uniq_labels, label_img)\n",
    "\n",
    "# plot result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "import scarlet.display\n",
    "norm = scarlet.display.LinearPercentileNorm(blend_images[i].sum(axis=-1), percentiles=[10,99])\n",
    "axes[0].imshow(scarlet.display.img_to_rgb(blend_images[i].sum(axis=-1), norm=norm))\n",
    "#axes[0].scatter(blend_list[i]['dx'], blend_list[i]['dy'], color='r', marker='x')\n",
    "for i,obj in enumerate(blend_list[i]):\n",
    "    axes[0].text(obj['dx'], obj['dy'], '{}'.format(i), color='r')\n",
    "axes[0].imshow(label_img, cmap='jet', alpha=0.5)\n",
    "\n",
    "\n",
    "# iou is ideally a one-hot encoding of the index of the matching source\n",
    "# the cluster label are randomly permutated, so compute cross-correlation matrix of true indices\n",
    "D = np.sqrt(Y.T @ Y)\n",
    "# use the sqaured deviation from identity as loss function\n",
    "_loss = ((D - np.eye(num_objects))**2).sum()\n",
    "axes[1].imshow(D, vmin=0, vmax=1)\n",
    "axes[1].set_xticks(np.arange(num_objects))\n",
    "axes[1].set_xticklabels(np.flatnonzero(has_object))\n",
    "axes[1].set_yticks(np.arange(num_objects))\n",
    "axes[1].set_yticks(np.arange(num_objects))\n",
    "print(_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the abive in single function\n",
    "def cl_loss(alpha, threshold, clusterer, img, fp_threshold=None, normalization=None):\n",
    "    # cluster data\n",
    "    X, mask = sim_matrix(img, threshold, alpha, normalization=normalization)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    uniq_labels = np.unique(labels)\n",
    "    label_img = np.ones(mask.shape) * -2\n",
    "    label_img[mask] = labels\n",
    "\n",
    "    # compare clustering label image to footprints\n",
    "    if fp_threshold is None:\n",
    "        fp_threshold = threshold\n",
    "\n",
    "    footprints = isolated_images[i].sum(axis=-1) > fp_threshold\n",
    "    Y = iou_matrix(footprints, uniq_labels, label_img)\n",
    "    D = np.sqrt(Y.T @ Y)\n",
    "    _loss = ((D - np.eye(D.shape[0]))**2).sum()\n",
    "    return _loss\n",
    "\n",
    "\n",
    "mcs = 10\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# compute detection mask and color/spatial distances\n",
    "threshold = std_sum * 5\n",
    "normalization = std_background[None,None,:]\n",
    "alpha = 5e-2\n",
    "img = blend_images[0]\n",
    "cl_loss(alpha, threshold, clusterer, img, fp_threshold=std_sum, normalization=normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform optimization of alpha and threshold\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# running this will cost your computer ~1 hour of its life...\n",
    "mcs = 10\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "loss = lambda p: np.sum([ cl_loss(p[0], p[1], clusterer, img, fp_threshold=std_sum, normalization=normalization) for img in blend_images ])\n",
    "minimize(loss, (1e-1, std_sum * 3), bounds=((0, 1), (std_sum, std_sum * 10)), options={'maxiter': 50, 'eps': (1e-2, std_sum)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now with mcs = 5\n",
    "mcs = 5\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "loss = lambda p: np.sum([ cl_loss(p[0], p[1], clusterer, img, fp_threshold=std_sum, normalization=normalization) for img in blend_images ])\n",
    "minimize(loss, (1e-1, std_sum * 3), bounds=((0, 1), (std_sum, std_sum * 10)), options={'maxiter': 50, 'eps': (1e-2, std_sum)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results with these settings are quite good for the fainter sources, but the algorithm is struggling with the overlap between brighter sources. In particular, detecting a moderately bright source in the footprint of a very bright source (like in #4) remains challenging. This problem is caused by having only one scaling term $\\alpha$ for the entire population. \n",
    "\n",
    "Brighter objects could probably work with a smaller setting of $\\alpha$ because the color information is very accurate, while small and faint source depends more heavily on the spatial distance information.\n",
    "\n",
    "As a result, one needs to either set $\\alpha$ adaptively or work with some multi-scale approach (evaluating at a grid of $\\alpha$ and then picking the best)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "from earlier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the detection efficiency of the HDBSCAN for a set of images with known numbers of clusters\n",
    "# function of minimum cluster size parameter\n",
    "\n",
    "# using parameter values from 5-50, in increments of 5\n",
    "min_cluster_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "# the detection efficiency using 'eom' on raw data\n",
    "eom_detection_efficiency = []\n",
    "# the detection efficiency using 'eom' on normalised data\n",
    "eom_norm_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on raw data\n",
    "leaf_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on normalised data\n",
    "leaf_norm_detection_efficiency = []\n",
    "\n",
    "# find the detection efficiency for each value used for the minimum cluster size parameter\n",
    "for mcs in min_cluster_sizes:\n",
    "    eom_efficiency = []\n",
    "    eom_norm_efficiency = []\n",
    "    leaf_efficiency = []\n",
    "    leaf_norm_efficiency = []\n",
    "    \n",
    "    # get results using both 'eom' and 'leaf' cluster selection methods\n",
    "    eom_clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs)\n",
    "    leaf_clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='leaf') \n",
    "\n",
    "    # get the average detection efficiency from each image\n",
    "    for i in range(len(blend_list)):\n",
    "        # get the actual number of clusters in this image\n",
    "        true_centers = np.stack([blend_list[i]['dx'], blend_list[i]['dy']]).T\n",
    "        actual_k = len(true_centers)\n",
    "                \n",
    "        # put each band along with x and y dimensions into one array\n",
    "        img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "        n = 120\n",
    "        x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "        arrays = [x.flatten(), y.flatten()]\n",
    "        for j in range(6):\n",
    "            arrays.append(img[:, j])\n",
    "        data = np.stack(arrays, axis=1)\n",
    "\n",
    "        # normalise data\n",
    "        norm_data = data.astype('float') #- background\n",
    "        norm_data /= (np.maximum(std_background, data.sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "    \n",
    "        # find number of clusters detected for raw data using 'eom'\n",
    "        eom_clusterer.fit(data)\n",
    "        detected_k = max(eom_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for normalised data using 'eom'\n",
    "        eom_clusterer.fit(norm_data)\n",
    "        detected_k = max(eom_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_norm_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for raw data using 'leaf'\n",
    "        leaf_clusterer.fit(data)\n",
    "        detected_k = max(leaf_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find number of clusters detected for normalised data using 'leaf'\n",
    "        leaf_clusterer.fit(data)\n",
    "        detected_k = max(leaf_clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_norm_efficiency.append(detected_k/actual_k)\n",
    "    \n",
    "    # take the efficiency to be the mean of the number of clusters detected divided by the actual number of clusters for each image\n",
    "    eom_detection_efficiency.append(np.mean(eom_efficiency))\n",
    "    eom_norm_detection_efficiency.append(np.mean(eom_norm_efficiency))\n",
    "    leaf_detection_efficiency.append(np.mean(leaf_efficiency))\n",
    "    leaf_norm_detection_efficiency.append(np.mean(leaf_norm_efficiency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of the minimum cluster size parameter\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(min_cluster_sizes, eom_detection_efficiency)\n",
    "eom_norm_sc = eom_norm_ax.scatter(min_cluster_sizes, eom_norm_detection_efficiency)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(min_cluster_sizes, leaf_detection_efficiency)\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(min_cluster_sizes, leaf_norm_detection_efficiency)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the detection efficiency of the HDBSCAN for a set of images with known numbers of clusters\n",
    "# function of number of clusters in imagee\n",
    "\n",
    "# using parameter values from 5-50, in increments of 5\n",
    "min_cluster_sizes = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "# the number of clusters in each image\n",
    "clusters = [] \n",
    "all_clusters = []\n",
    "# the detection efficiency using 'eom' on raw data\n",
    "eom_detection_efficiency = []\n",
    "total_eom_detection_efficiency = []\n",
    "# the detection efficiency using 'eom' on normalised data\n",
    "eom_norm_detection_efficiency = []\n",
    "total_eom_norm_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on raw data\n",
    "leaf_detection_efficiency = []\n",
    "total_leaf_detection_efficiency = []\n",
    "# the detection efficiency using 'leaf' on normalised data\n",
    "leaf_norm_detection_efficiency = []\n",
    "total_leaf_norm_detection_efficiency = []\n",
    "\n",
    "# find the average number of clusters detected using HDBSCAN for each image\n",
    "for i in range(len(blend_list)):\n",
    "    eom_efficiency = []\n",
    "    eom_norm_efficiency = []\n",
    "    leaf_efficiency = []\n",
    "    leaf_norm_efficiency = []\n",
    "    \n",
    "    # put each band along with x and y dimensions into one array\n",
    "    img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "    x, y = np.meshgrid(0.1*np.arange(120), 0.1*np.arange(120))\n",
    "    arrays = [x.flatten(), y.flatten()]\n",
    "    for j in range(6):\n",
    "        arrays.append(img[:, j])\n",
    "    data = np.stack(arrays, axis=1)\n",
    "\n",
    "    # normalise data\n",
    "    norm_data = data.astype('float') #- background # works much better without subtracting the background- why? \n",
    "    norm_data /= (np.maximum(std_background, data.sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "\n",
    "    # get the actual number of clusters from the image\n",
    "    true_centers = np.stack([blend_list[i]['dx'], blend_list[i]['dy']]).T\n",
    "    actual_k = len(true_centers)\n",
    "    clusters.append(actual_k)\n",
    "\n",
    "    # use HDBSCAN to estimate the number of clusters using a series of values for the minimum cluster size parameter\n",
    "    for mcs in min_cluster_sizes:\n",
    "        all_clusters.append(actual_k)\n",
    "        \n",
    "        # clustering using HDBSCAN with the 'eom' selection method\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='eom')\n",
    "        \n",
    "        # find the number of clusters using raw data\n",
    "        clusterer.fit(data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_efficiency.append(detected_k)\n",
    "        total_eom_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find the number of clusters using normalised data\n",
    "        clusterer.fit(norm_data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        eom_norm_efficiency.append(detected_k)\n",
    "        total_eom_norm_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # clustering using HDBSCAN with the 'leaf' selection method\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, cluster_selection_method='leaf')\n",
    "        \n",
    "        # find the number of clusters using raw data\n",
    "        clusterer.fit(data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1 \n",
    "        leaf_efficiency.append(detected_k)\n",
    "        total_leaf_detection_efficiency.append(detected_k/actual_k)\n",
    "        \n",
    "        # find the number of clusters using normalised data\n",
    "        clusterer.fit(norm_data)\n",
    "        detected_k = max(clusterer.labels_)   # number of distinct things- noise as a cluster\n",
    "        if (detected_k == -1): detected_k = 0 # does labelling start at 0 or 1\n",
    "        leaf_norm_efficiency.append(detected_k)\n",
    "        total_leaf_norm_detection_efficiency.append(detected_k/actual_k)\n",
    "    \n",
    "    # take the efficiency to be the mean of the number of clusters detected divided by the actual number of clusters for every image\n",
    "    eom_detection_efficiency.append(np.mean(eom_efficiency)/actual_k)\n",
    "    eom_norm_detection_efficiency.append(np.mean(eom_norm_efficiency)/actual_k)\n",
    "    leaf_detection_efficiency.append(np.mean(leaf_efficiency)/actual_k)\n",
    "    leaf_norm_detection_efficiency.append(np.mean(leaf_norm_efficiency)/actual_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of actual number of clusters\n",
    "# using a colour map to show the average magnitude of the galaxies in an image\n",
    "cm = plt.cm.get_cmap('RdYlBu')\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(clusters, eom_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "eom_norm_sc = eom_norm_ax.scatter(clusters, eom_norm_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(eom_sc, ax=eom_ax)\n",
    "eom_cb = plt.colorbar(eom_norm_sc, ax=eom_norm_ax)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(clusters, leaf_detection_efficiency, c=magnitudes, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(clusters, leaf_norm_detection_efficiency, c=magnitudes, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(leaf_sc, ax=leaf_ax)\n",
    "leaf_cb = plt.colorbar(leaf_norm_sc, ax=leaf_norm_ax)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_cb.set_label(\"Average Magnitude of Clusters\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_cb.set_label(\"Average Magnitude of Clusters\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the detection efficiency as a function of actual number of clusters for every parameter value\n",
    "# using a colour map to show the minimum cluster size parameter\n",
    "cm = plt.cm.get_cmap('Spectral')\n",
    "min_cluster_vals = [mc for mc in min_cluster_sizes for i in range(len(blend_list))]\n",
    "\n",
    "# plotting raw and normalised data using 'eom' selection method\n",
    "eom_fig = plt.figure()\n",
    "eom_ax = eom_fig.add_subplot(121)\n",
    "eom_norm_ax = eom_fig.add_subplot(122)\n",
    "eom_sc = eom_ax.scatter(all_clusters, total_eom_detection_efficiency, c=min_cluster_vals, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "eom_norm_sc = eom_norm_ax.scatter(all_clusters, total_eom_norm_detection_efficiency, c=min_cluster_vals, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(eom_sc, ax=eom_ax)\n",
    "eom_cb = plt.colorbar(eom_norm_sc, ax=eom_norm_ax)\n",
    "\n",
    "# plotting raw and normalised data using 'leaf' selection method\n",
    "leaf_fig = plt.figure()\n",
    "leaf_ax = leaf_fig.add_subplot(121)\n",
    "leaf_norm_ax = leaf_fig.add_subplot(122)\n",
    "leaf_sc = leaf_ax.scatter(all_clusters, total_leaf_detection_efficiency, c=min_cluster_vals, cmap=cm)  # magnitudes vs bulge magnitudes\n",
    "leaf_norm_sc = leaf_norm_ax.scatter(all_clusters, total_leaf_norm_detection_efficiency, c=min_cluster_vals, cmap=cm)\n",
    "# add average ratio for number of clusters?\n",
    "plt.colorbar(leaf_sc, ax=leaf_ax)\n",
    "leaf_cb = plt.colorbar(leaf_norm_sc, ax=leaf_norm_ax)\n",
    "\n",
    "# labelling and organising 'eom' axes \n",
    "eom_ax.set_title(\"EOM Selection on Raw Data\")\n",
    "eom_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_cb.set_label(\"Minimum Cluster Size Parameter Value\")\n",
    "eom_norm_ax.set_title(\"EOM Selection on Normalised Data\")\n",
    "eom_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "eom_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "eom_fig.tight_layout()\n",
    "\n",
    "# labelling and organising 'leaf' axes\n",
    "leaf_ax.set_title(\"Leaf Selection on Raw Data\")\n",
    "leaf_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_cb.set_label(\"Minimum Cluster Size Parameter Value\")\n",
    "leaf_norm_ax.set_title(\"Leaf Selection on Normalised Data\")\n",
    "leaf_norm_ax.set_ylabel(\"Ratio of Detected Clusters to Actual Clusters\")\n",
    "leaf_norm_ax.set_xlabel(\"Actual Number of Clusters\")\n",
    "leaf_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background, std_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = blend_images[0:1][0].reshape(-1, 6)\n",
    "n = 120\n",
    "x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "arrays = [x.flatten(), y.flatten()]\n",
    "for j in range(6):\n",
    "    arrays.append(img[:, j])\n",
    "data = np.stack(arrays, axis=1)\n",
    "plt.imshow(img.reshape(n, n, 6)[:, :, 0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing the clustering results for 5 different images\n",
    "\n",
    "# code for generating galaxies: https://github.com/LSSTDESC/BlendingToolKit/blob/master/notebooks/custom_sampling_function.ipynb\n",
    "# input catalog name\n",
    "catalog_name = os.path.join(os.path.dirname(os.getcwd()), 'data', 'sample_input_catalog.fits')\n",
    "\n",
    "# load parameters\n",
    "param = btk.config.Simulation_params(catalog_name, max_number=10, batch_size=5)\n",
    "np.random.seed(param.seed)\n",
    "\n",
    "# load input catalog\n",
    "catalog = btk.get_input_catalog.load_catalog(param)\n",
    "\n",
    "# generate catalogs of blended objects \n",
    "blend_generator = btk.create_blend_generator.generate(param, catalog)\n",
    "\n",
    "# generates observing conditions for the selected survey_name and all input bands\n",
    "observing_generator = btk.create_observing_generator.generate(param)\n",
    "\n",
    "# generate images of blends in all the observing bands\n",
    "draw_blend_generator = btk.draw_blends.generate(param, blend_generator, observing_generator)\n",
    "\n",
    "# generates new batch_size number of blends\n",
    "blend_results = next(draw_blend_generator)\n",
    "output = blend_results\n",
    "blend_images = output['blend_images']\n",
    "isolated_images = output['isolated_images']\n",
    "blend_list = output['blend_list']\n",
    "obs_cond = output['obs_condition']\n",
    "\n",
    "# get clustering results using 'leaf' selection method and a minimum cluster size of 10 for normalised data\n",
    "for i in range(len(blend_list)):\n",
    "    # put each band along with x and y dimensions into one array\n",
    "    img = blend_images[i:i+1][0].reshape(-1, 6)\n",
    "    n = 120\n",
    "    x, y = np.meshgrid(0.1*np.arange(n), 0.1*np.arange(n))\n",
    "    arrays = [x.flatten(), y.flatten()]\n",
    "    for j in range(6):\n",
    "        arrays.append(img[:, j])\n",
    "    data = np.stack(arrays, axis=1)\n",
    "\n",
    "    # normalise data\n",
    "    norm_data = data.astype('float') #- background # works much better without subtracting the background- why? \n",
    "    mask = norm_data[:, 2:].sum(axis=1) < std_background\n",
    "    norm_data[:, 2:] /= (np.maximum(std_background, norm_data[:, 2:].sum(axis=1)[:, None])) # reducing number of radical outliers\n",
    "    norm_data[:, 2:][mask] = 0\n",
    "    #norm_data[:, :2] /= n\n",
    "\n",
    "    # use HDBSCAN to get a clustering result\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=10, cluster_selection_method='leaf')\n",
    "    clusterer.fit(norm_data)\n",
    "    labels = clusterer.labels_.reshape(n, n)\n",
    "    \n",
    "    # plot images\n",
    "    fig = plt.figure()\n",
    "    ax_data = fig.add_subplot(121)\n",
    "    ax_hdb = fig.add_subplot(122)\n",
    "    ax_data.imshow(blend_images[i:i+1][0][:, :, :].sum(axis=-1), origin='lower')\n",
    "    #ax_data.imshow(norm_data.sum(axis=-1).reshape(n,n), origin='lower')\n",
    "    ax_hdb.imshow(labels, origin='lower')\n",
    "    #ax_hdb.hist(norm_data.sum(axis=-1).flatten(), bins=100)\n",
    "    \n",
    "    #btk.plot_utils.plot_blends(blend_images[i:i+1], blend_list[i:i+1], limits=(30,90))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btk.plot_utils.plot_blends(blend_images, blend_list, limits=(30,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
