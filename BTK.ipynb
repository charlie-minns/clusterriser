{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import astropy.table\n",
    "import scarlet\n",
    "import hdbscan\n",
    "sys.path.insert(0,os.path.dirname(os.getcwd()))\n",
    "import btk\n",
    "import btk.config, btk.plot_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for generating galaxies: https://github.com/LSSTDESC/BlendingToolKit/blob/master/notebooks/custom_sampling_function.ipynb\n",
    "# input catalog name\n",
    "catalog_name = os.path.join(os.path.dirname(os.getcwd()), 'data', 'sample_input_catalog.fits')\n",
    "\n",
    "# load parameters\n",
    "# max_number = maximum number of galaxies in image, batch_size = number of images\n",
    "param = btk.config.Simulation_params(catalog_name, max_number=10, batch_size=100)\n",
    "np.random.seed(param.seed)\n",
    "\n",
    "# load input catalog\n",
    "catalog = btk.get_input_catalog.load_catalog(param)\n",
    "\n",
    "# generate catalogs of blended objects \n",
    "blend_generator = btk.create_blend_generator.generate(param, catalog)\n",
    "\n",
    "# generates observing conditions for the selected survey_name and all input bands\n",
    "observing_generator = btk.create_observing_generator.generate(param)\n",
    "\n",
    "# generate images of blends in all the observing bands\n",
    "draw_blend_generator = btk.draw_blends.generate(param, blend_generator, observing_generator)\n",
    "\n",
    "# generates new batch_size number of blends\n",
    "blend_results = next(draw_blend_generator)\n",
    "output = blend_results\n",
    "blend_images = output['blend_images']\n",
    "isolated_images = output['isolated_images']\n",
    "blend_list = output['blend_list']\n",
    "obs_cond = output['obs_condition']\n",
    "\n",
    "# plot blended images\n",
    "plot = 0\n",
    "btk.plot_utils.plot_blends(blend_images[0:plot], blend_list[0:plot], limits=(30,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the background and noise of the images\n",
    "sky_level = []\n",
    "for oc in obs_cond[0]: \n",
    "    sky_level.append(oc.mean_sky_level)\n",
    "background = np.array(sky_level)\n",
    "std_background = np.sqrt(background)\n",
    "std_sum = np.sqrt((std_background**2).sum())\n",
    "\n",
    "# show histogram comparison, single band only\n",
    "bins = np.linspace(-3*std_background[0], 3*std_background[0], 50)\n",
    "plt.hist(blend_images[:, :, :, 0].flatten(), bins=bins, density=True);\n",
    "plt.plot(bins, norm.pdf(bins, scale=std_background[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data: sum normalization for band amplitudes\n",
    "def normalize_channels(img, std_sum):\n",
    "    # prevent division by zero (or close to) by cutting normalization off at noise level\n",
    "    return img / np.maximum(std_sum, img.sum(axis=-1))[:, :, None]\n",
    "\n",
    "# function for putting data into a form for hdbscan \n",
    "def hdbscan_data(img, threshold, alpha=1):\n",
    "    # select pixels whose sum is above threshold,\n",
    "    Ny, Nx, C = img.shape\n",
    "    mask = img.sum(axis=-1) > threshold\n",
    "    # normalize their intensities, and xy values multiplied with alpha to extend feature vector\n",
    "    img_ = normalize_channels(img, threshold)\n",
    "    \n",
    "    # append data to array with spatial information\n",
    "    x, y = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "    arrays = [alpha*x.flatten(), alpha*y.flatten()] + [img_[:, :, c].flatten() for c in range(C)]\n",
    "    data = np.stack(arrays, axis=1)\n",
    "    return data, mask\n",
    "\n",
    "# get clustering result with HDBSCAN\n",
    "i = 0    # index of image to analyse\n",
    "Ny, Nx, C = blend_images[i].shape\n",
    "mcs = 10\n",
    "data, mask = hdbscan_data(blend_images[i], std_sum*3, alpha=0.01)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=mcs, min_samples=5, cluster_selection_method='leaf', allow_single_cluster=True)\n",
    "labels = clusterer.fit_predict(data)\n",
    "clusters = np.unique(labels)\n",
    "k = clusters.shape\n",
    "print(\"Number of clusters: \" + str(k))\n",
    "\n",
    "# create mask for data\n",
    "labels_ma = np.ma.array(labels, mask=~mask)\n",
    "\n",
    "# plot result\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,6))\n",
    "axes[0].imshow(blend_images[i].sum(axis=-1), origin='lower')\n",
    "axes[0].scatter(blend_list[i]['dx'], blend_list[i]['dy'], color='r', marker='x')\n",
    "axes[1].imshow(mask, cmap='gray', origin='lower')\n",
    "axes[1].imshow(labels_ma.reshape(mask.shape), cmap='jet', origin='lower')\n",
    "\n",
    "# label plots\n",
    "axes[0].set_title(\"6-band Image with \" + str(len(blend_list[i])) + \" Centers\")\n",
    "axes[0].axis('off')\n",
    "axes[1].set_title(\"HDBSCAN Clustering Result\")\n",
    "axes[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the detection mask and color/spatial distances\n",
    "def sim_matrix(img, threshold, alpha, normalisation=None):\n",
    "    # work on pixels above threshold and normalise data\n",
    "    mask = img.sum(axis=-1) > threshold\n",
    "    if normalisation is None: _img = img\n",
    "    else: _img = img / normalisation\n",
    "        \n",
    "    # compute Pearson r for color distance\n",
    "    Ny, Nx, C = img.shape\n",
    "    r = np.corrcoef(_img[mask, :].reshape(-1, C))\n",
    "    \n",
    "    # compute pairwise Euclidean distance\n",
    "    x, y = np.meshgrid(np.arange(Nx), np.arange(Ny))\n",
    "    xy = np.stack((x[mask].flatten(), y[mask].flatten()), axis=1)\n",
    "    R2 = ((xy[:, None] - xy[None, :])**2).sum(axis=-1)/2\n",
    "\n",
    "    # combine color and spatial distance, with alpha scaling\n",
    "    dist = (1-r) + R2*alpha**2\n",
    "    return dist, mask\n",
    "\n",
    "# computes intersection over union for every pair of true and clustered\n",
    "def iou_matrix(footprints, clusters, label_img):\n",
    "    has_object = footprints.any(axis=(1, 2))\n",
    "    num_objects = has_object.sum()\n",
    "    if clusters[0] == -1: k = len(clusters)-1\n",
    "    else: k = len(clusters)\n",
    "    iou = np.zeros((k, num_objects))\n",
    "    for ll in clusters:\n",
    "        if ll >= 0:\n",
    "            fp_label = label_img == ll\n",
    "            for ii, fp_true in enumerate(footprints[has_object]):\n",
    "                union = (fp_true | fp_label).sum()\n",
    "                intersection = (fp_true & fp_label).sum()\n",
    "                norm = np.sqrt(fp_true.sum() * fp_label.sum())\n",
    "                iou_ = intersection/union\n",
    "                iou[ll][ii] = iou_\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the loss value for a given alpha and threshold for a given image\n",
    "def cl_loss(alpha, threshold, clusterer, img, fp_threshold=None, normalisation=None):\n",
    "    # cluster data\n",
    "    X, mask = sim_matrix(img, threshold, alpha, normalisation=normalisation)\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    clusters = np.unique(labels)\n",
    "    label_img = -2*np.ones(mask.shape)\n",
    "    label_img[mask] = labels\n",
    "\n",
    "    # compare clustering label image to footprints\n",
    "    if fp_threshold is None: fp_threshold = threshold\n",
    "    footprints = isolated_images[i].sum(axis=-1) > fp_threshold\n",
    "    Y = iou_matrix(footprints, clusters, label_img)\n",
    "    D = np.sqrt(Y.T @ Y)\n",
    "    _loss = ((D - np.eye(D.shape[0]))**2).sum()\n",
    "    return _loss\n",
    "\n",
    "# test cl_loss function\n",
    "mcs = 10\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# compute detection mask and color/spatial distances\n",
    "threshold = std_sum*5\n",
    "normalisation = std_background[None, None, :]\n",
    "alpha = 5e-2\n",
    "img = blend_images[0]\n",
    "loss_value = cl_loss(alpha, threshold, clusterer, img, fp_threshold=std_sum, normalisation=normalisation)\n",
    "print(\"Loss value: \" + str(loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform optimisation of alpha and threshold using minimum cluster size of 10 (takes ~1 hour)\n",
    "mcs = 10\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# find optimal values from images in blend_images\n",
    "loss = lambda p: np.sum([cl_loss(p[0], p[1], clusterer, img, fp_threshold=std_sum, normalisation=normalisation) for img in blend_images])\n",
    "mcs10 = minimize(loss, (1e-1, std_sum*3), bounds=((0, 1), (std_sum, std_sum*10)), options={'maxiter': 50, 'eps': (1e-2, std_sum)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisation with minimum cluster size of 5 (takes ~1 hour)\n",
    "mcs = 5\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# find optimal values from images in blend_images\n",
    "loss = lambda p: np.sum([cl_loss(p[0], p[1], clusterer, img, fp_threshold=std_sum, normalisation=normalisation) for img in blend_images])\n",
    "mcs5 = minimize(loss, (1e-1, std_sum*3), bounds=((0, 1), (std_sum, std_sum*10)), options={'maxiter': 50, 'eps': (1e-2, std_sum)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysing optimal results on image at index i\n",
    "i = 0\n",
    "mcs = 10\n",
    "clusterer = hdbscan.HDBSCAN(metric='precomputed', \n",
    "                            min_cluster_size=mcs, \n",
    "                            min_samples=1, \n",
    "                            cluster_selection_method='eom',\n",
    "                            allow_single_cluster=True,\n",
    "                           )\n",
    "\n",
    "# alpha and threshold values from optimimzation below\n",
    "# can pick any positive number for alpha, and something of order std_sum for threshold\n",
    "if mcs == 10:\n",
    "    alpha, threshold = mcs10x['x'] \n",
    "elif mcs == 5:\n",
    "    alpha, threshold = mcs5x['x'] \n",
    "print(\"Alpha: \" + str(alpha))\n",
    "print(\"Threshold: \" + str(threshold))\n",
    "\n",
    "# all channels unit variance\n",
    "normalisation = std_background[None, None, :]\n",
    "    \n",
    "# compute detection mask and color/spatial distances\n",
    "X, mask = sim_matrix(blend_images[i], threshold, alpha, normalisation=normalisation)\n",
    "\n",
    "# cluster distance matrix\n",
    "labels = clusterer.fit_predict(X)\n",
    "clusters = np.unique(labels)\n",
    "k = clusters.shape\n",
    "print(\"Number of clusters: \" + str(k))\n",
    "label_img = -2*np.ones(mask.shape)\n",
    "label_img[mask] = labels\n",
    "label_ma = np.ma.array(label_img, mask=~mask)\n",
    "\n",
    "# check overlap with true footprints\n",
    "footprints = isolated_images[i].sum(axis=-1) > std_sum\n",
    "has_object = footprints.any(axis=(1, 2))\n",
    "num_objects = has_object.sum()\n",
    "Y = iou_matrix(footprints, clusters, label_img)\n",
    "\n",
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,6))\n",
    "norm = scarlet.LinearPercentileNorm(blend_images[i].sum(axis=-1), percentiles=[10, 99])\n",
    "ax[0].imshow(blend_images[i].sum(axis=-1), origin='lower')\n",
    "\n",
    "# label cluster centres numerically\n",
    "for j, obj in enumerate(blend_list[i]):\n",
    "    ax[0].text(obj['dx'], obj['dy'], '{}'.format(j), color='r')\n",
    "    ax[1].text(obj['dx'], obj['dy'], '{}'.format(j), color='r')\n",
    "ax[1].imshow(mask, cmap='gray', origin='lower')\n",
    "ax[1].imshow(label_ma, cmap='jet', alpha=0.9, origin='lower')\n",
    "\n",
    "# label plots\n",
    "ax[0].set_title(\"6-band Image with \" + str(len(blend_list[i])) + \" Centers\")\n",
    "ax[0].axis('off')\n",
    "ax[1].set_title(\"HDBSCAN Clustering Result\")\n",
    "ax[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the loss value to plot the similarities between detected and true clusters\n",
    "# iou is ideally a one-hot encoding of the index of the matching source\n",
    "# the cluster label are randomly permutated, so compute cross-correlation matrix of true indices\n",
    "D = np.sqrt(Y.T @ Y)\n",
    "# use the squared deviation from identity as loss function\n",
    "_loss = ((D - np.eye(num_objects))**2).sum()\n",
    "print(\"Loss value: \" + str(_loss))\n",
    "\n",
    "# plot result\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cm = ax.imshow(D, vmin=0, vmax=1, cmap='jet')\n",
    "cbar = fig.colorbar(cm, ax=ax)\n",
    "ax.set_xticks(np.arange(num_objects))\n",
    "ax.set_xticklabels(np.flatnonzero(has_object))\n",
    "ax.set_yticks(np.arange(num_objects))\n",
    "ax.set_yticks(np.arange(num_objects))\n",
    "\n",
    "# label plot\n",
    "ax.set_title(\"Similarity Between Clusters\")\n",
    "ax.set_xlabel(\"True Cluster Index\")\n",
    "ax.set_ylabel(\"Detected Cluster Index\")\n",
    "cbar.set_label(\"Similarity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
